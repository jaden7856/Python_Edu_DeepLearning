{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention 신경망 구현 및 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* konlp 설치 관련해서 OS관련해서 추가 설치들이 필요하니, 관련 내용 잘 확인 필요!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/img_41.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tag 여러 종류  \n",
    "  - JHannanum is a morphological analyzer and POS tagger written in Java, and developed by the Semantic Web Research Center (SWRC) at KAIST since 1999   \n",
    "  - Kkma is a morphological analyzer and natural language processing system written in Java, developed by the Intelligent Data Systems (IDS) Laboratory at SNU.    \n",
    "  - KOMORAN is a relatively new open source Korean morphological analyzer written in Java, developed by Shineware, since 2013.    \n",
    "  - MeCab, originally a Japanese morphological analyzer and POS tagger developed by the Graduate School of Informatics in Kyoto University, was modified to MeCab-ko by the Eunjeon Project to adapt to the Korean language.    \n",
    "  - Open Korean Text is an open source Korean tokenizer written in Scala, developed by Will Hohyon Ryu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "import jpype\n",
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혹시 konlp가 제대로 설치 되었는지 확인을 위해서...\n",
    "# 위의 konlpy만 해서는 제대로 설치 되었는지 확인이 안 될 수 있음!!!!\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "NUM_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 입력이 one-hot-encdoing 형식으로 들어오면 embedding을 수행을 먼저\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
    "        # 그리고 LSTM에서 hidden state을 출력으로 던져주어야 이것을 활용해서 return을 활용할 수 있다...\n",
    "        # return_sequence=True : 디코더에서 나오는 것 하나하나 알아야 하기에...\n",
    "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
    "\n",
    "    # 인코더의 경우에는 입력이 들어오면 이것을 바탕으로 히든, 셀 스테이트를 하도록 ...\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.emb(x)\n",
    "        # h,c는 test를 할 경우에는 다음으로 넘겨 주어야 forward  로 계산을 하면서 나갈 수 있으니..\n",
    "        H, h, c = self.lstm(x)\n",
    "        return H, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
    "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
    "        \n",
    "        #****Attention을 추가함!!!! \n",
    "        self.att = tf.keras.layers.Attention()\n",
    "        \n",
    "        # 최종 단어 단에서 어떤 것을 할 것인지..선택..\n",
    "        self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # ***처음 들어오는 인코더 단에서 넘어오는 것들을 받고,,\n",
    "        x, s0, c0, H = inputs\n",
    "        x = self.emb(x)\n",
    "        # ****S는 hidden state를 모두 모아둔 부분..--> 쿼리로 사용을 할 것임....--->하나 앞선 시간을 사용을 해서..\n",
    "        S, h, c = self.lstm(x, initial_state=[s0, c0])\n",
    "        # ***쿼리로 사용을 할 것임....--->하나 앞선 시간을 사용을 해서..(1차원을 3차원으로 확장) & 마지막 히든은 제외...해서 :-1까지..\n",
    "        S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1)\n",
    "        # ****키와 value를 계산하고,,,\n",
    "        A = self.att([S_, H])\n",
    "        y = tf.concat([S, A], axis=-1)\n",
    "        \n",
    "        return self.dense(y), h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고 사항   \n",
    " - super : 자식클래스 내에서 코드에서도 부모클래스를 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(tf.keras.Model):\n",
    "    def __init__(self, sos, eos):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        # 1) 가장 기본적으로 필요한 Encoder, Deoder, sos, eos에 관련된 부분을 지정을 함!!!!\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "    \n",
    "    # 연결에서 구성 --> 학습을 위해서는 입력/출력 모두 알아야 하기에 x,y = input으로 받음\n",
    "    # 그리고 디코더에 입력을 넣어주어야 하기에..\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "       # (학습 과정)\n",
    "        if training is True:\n",
    "            # 학습을 위해서는 입력/출력 모두 알아야 하기에 x,y = input으로 받음\n",
    "            # 그리고 디코더에 입력을 넣어주어야 하기에..\n",
    "            x, y = inputs\n",
    "            # encoder에 입력  x를 넣어서 나온 hidden state, cell  (LSTM으로 구현이 되어서..)\n",
    "            H, h, c = self.enc(x)\n",
    "            # 최종 출력 y를 하도록...\n",
    "            y, _, _ = self.dec((y, h, c, H))\n",
    "            return y\n",
    "        # (Test 과정) --> 그러니 정답  y는 없음..\n",
    "        else:\n",
    "            x = inputs\n",
    "            H, h, c = self.enc(x)\n",
    "            # 뒤에 디커더 부분에 입력을 넣어 주는 부분이 달라지게 된다!!!! --> sos  를 넣어준다.\n",
    "            y = tf.convert_to_tensor(self.sos)\n",
    "            y = tf.reshape(y, (1, 1))\n",
    "\n",
    "            # 최대 seq는 64길이까지만..\n",
    "            seq = tf.TensorArray(tf.int32, 64)\n",
    "            \n",
    "            # tf의  for loop으로 사용을 해서 최대 64까지 \n",
    "            for idx in tf.range(64):\n",
    "                # for 에서 처음에는 처음으로 만들어준 sos를 decoder에 입력으로 넣어주고 --> self.dec(y,h,c,H)을 해서\n",
    "                # 출력인 y와 hidden state, cell state를 출력으로 준다.\n",
    "                y, h, c = self.dec([y, h, c, H])\n",
    "                # 가장 큰 y값에 대한 index를 얻어오게 되는 과정\n",
    "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
    "                # reshape를 하면서 batch를 사용을 하기 위해서...\n",
    "                y = tf.reshape(y, (1, 1))\n",
    "                # seq를 하나씩 받으면서 처리하기 위해서...\n",
    "                seq = seq.write(idx, y)\n",
    "                # eos일 때 까지 for lopp순환\n",
    "                if y == self.eos:\n",
    "                    break\n",
    "            # \n",
    "            return tf.reshape(seq.stack(), (1, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습, 테스트 루프 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/img_40.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement training loop\n",
    "@tf.function\n",
    "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    # 마지맞 eos는 있고, 처음 sos 는 없는 것...\n",
    "    output_labels = labels[:, 1:]\n",
    "    # 처음 sos는 포함이 되고, 마지막 eos는 빼고...\n",
    "    shifted_labels = labels[:, :-1]\n",
    "    # 그래서 위의 시프트 된 것들이 아래의 model의 학습 과정에 들어가게 된다!!!!!\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 위의 output_labels, shifted_labels 이 들어가게 된다...\n",
    "        predictions = model([inputs, shifted_labels], training=True)\n",
    "        loss = loss_object(output_labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(output_labels, predictions)\n",
    "\n",
    "# Implement algorithm test\n",
    "@tf.function\n",
    "def test_step(model, inputs):\n",
    "    return model(inputs, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-A 형태의 데이터 셋... ---> 본인들이 구성을 하시면 됩니다!!!!\n",
    "# cafe.xlsx를 바탕으로 한 것이고, 다른 데이터 셋도 구할 수 있습니다!!!!!\n",
    "dataset_file = 'chatbot_data_sample_2.csv' # acquired from 'http://www.aihub.or.kr' and modified\n",
    "okt = Okt()\n",
    "\n",
    "with open(dataset_file, 'r',encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    # 한 줄에 대한 형태소 분석 수행-->morphs\n",
    "    seq = [' '.join(okt.morphs(line)) for line in lines]\n",
    "\n",
    "# 원본 데이터에서 질문과 답변에 대한 것들을 분리...한줄 한줄 건너서 있으니...처음에 질문부터이니 0부터 질문, 홀수가 답변--> 하나씩 건너서..\n",
    "questions = seq[::2]\n",
    "answers = ['\\t ' + lines for lines in seq[1::2]]\n",
    "\n",
    "num_sample = len(questions)\n",
    "\n",
    "# 질문을 보고 한 번 데이터 셋을 섞어준다..\n",
    "perm = list(range(num_sample))\n",
    "random.seed(0)\n",
    "random.shuffle(perm)\n",
    "# 섞으면서 train/ test로 구분을 하기 위한 것..\n",
    "train_q = list()\n",
    "train_a = list()\n",
    "test_q = list()\n",
    "test_a = list()\n",
    "\n",
    "# 질문과 답변에 대한 것을 돌면서...\n",
    "for idx, qna in enumerate(zip(questions, answers)):\n",
    "    q, a = qna\n",
    "    # 5/1은 test, 5/4는 train\n",
    "    if perm[idx] > num_sample//5:\n",
    "        train_q.append(q)\n",
    "        train_a.append(a)\n",
    "    else:\n",
    "        test_q.append(q)\n",
    "        test_a.append(a)\n",
    "\n",
    "# 문장에서 짤라주는 것에 대한 세팅...\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "\n",
    "tokenizer.fit_on_texts(train_q + train_a)\n",
    "\n",
    "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
    "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
    "\n",
    "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
    "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
    "\n",
    "# 입력은 뒤로...패딩..\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=64)\n",
    "# 출력은 앞에로 패딩..\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=65)\n",
    "\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=64)\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=65)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 환경 정의\n",
    "### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Seq2seq(sos=tokenizer.word_index['\\t'],\n",
    "                eos=tokenizer.word_index['\\n'])\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define performance metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.0906476974487305, Accuracy: 72.18946838378906\n",
      "Epoch 2, Loss: 0.8242252469062805, Accuracy: 90.49724578857422\n",
      "Epoch 3, Loss: 0.6410475373268127, Accuracy: 90.59552001953125\n",
      "Epoch 4, Loss: 0.5597950220108032, Accuracy: 91.11634826660156\n",
      "Epoch 5, Loss: 0.5483094453811646, Accuracy: 91.29324340820312\n",
      "Epoch 6, Loss: 0.5363156199455261, Accuracy: 90.6446533203125\n",
      "Epoch 7, Loss: 0.5181447863578796, Accuracy: 91.16548919677734\n",
      "Epoch 8, Loss: 0.5107572078704834, Accuracy: 91.22444915771484\n",
      "Epoch 9, Loss: 0.5061613917350769, Accuracy: 91.41116333007812\n",
      "Epoch 10, Loss: 0.5029914975166321, Accuracy: 91.41116333007812\n",
      "Epoch 11, Loss: 0.49557772278785706, Accuracy: 91.17530822753906\n",
      "Epoch 12, Loss: 0.49373659491539, Accuracy: 91.1949691772461\n",
      "Epoch 13, Loss: 0.49131107330322266, Accuracy: 91.41116333007812\n",
      "Epoch 14, Loss: 0.48679834604263306, Accuracy: 91.14582824707031\n",
      "Epoch 15, Loss: 0.4828869700431824, Accuracy: 91.30307006835938\n",
      "Epoch 16, Loss: 0.4797080159187317, Accuracy: 91.33255004882812\n",
      "Epoch 17, Loss: 0.4773375988006592, Accuracy: 91.22444915771484\n",
      "Epoch 18, Loss: 0.474583238363266, Accuracy: 91.45046997070312\n",
      "Epoch 19, Loss: 0.47224363684654236, Accuracy: 91.34236907958984\n",
      "Epoch 20, Loss: 0.4662299156188965, Accuracy: 91.46029663085938\n",
      "Epoch 21, Loss: 0.4616658091545105, Accuracy: 91.5389175415039\n",
      "Epoch 22, Loss: 0.4589077830314636, Accuracy: 91.46029663085938\n",
      "Epoch 23, Loss: 0.4541979432106018, Accuracy: 91.51925659179688\n",
      "Epoch 24, Loss: 0.4478488564491272, Accuracy: 91.46029663085938\n",
      "Epoch 25, Loss: 0.44035911560058594, Accuracy: 91.5389175415039\n",
      "Epoch 26, Loss: 0.43203386664390564, Accuracy: 91.47013092041016\n",
      "Epoch 27, Loss: 0.42404597997665405, Accuracy: 91.78459167480469\n",
      "Epoch 28, Loss: 0.41679105162620544, Accuracy: 91.70597076416016\n",
      "Epoch 29, Loss: 0.4092436730861664, Accuracy: 92.05974578857422\n",
      "Epoch 30, Loss: 0.4040301442146301, Accuracy: 92.04009246826172\n",
      "Epoch 31, Loss: 0.3978044092655182, Accuracy: 92.19732666015625\n",
      "Epoch 32, Loss: 0.3944365978240967, Accuracy: 92.33490753173828\n",
      "Epoch 33, Loss: 0.3896978199481964, Accuracy: 92.27593994140625\n",
      "Epoch 34, Loss: 0.3861975371837616, Accuracy: 92.48230743408203\n",
      "Epoch 35, Loss: 0.3824273645877838, Accuracy: 92.57075500488281\n",
      "Epoch 36, Loss: 0.3780827522277832, Accuracy: 92.60023498535156\n",
      "Epoch 37, Loss: 0.3744851350784302, Accuracy: 92.61006164550781\n",
      "Epoch 38, Loss: 0.37114205956459045, Accuracy: 92.6985092163086\n",
      "Epoch 39, Loss: 0.3671573996543884, Accuracy: 92.72798919677734\n",
      "Epoch 40, Loss: 0.36432942748069763, Accuracy: 92.86557006835938\n",
      "Epoch 41, Loss: 0.3604816496372223, Accuracy: 92.84590911865234\n",
      "Epoch 42, Loss: 0.3575279116630554, Accuracy: 92.91470336914062\n",
      "Epoch 43, Loss: 0.35541969537734985, Accuracy: 92.86557006835938\n",
      "Epoch 44, Loss: 0.351487398147583, Accuracy: 92.99331665039062\n",
      "Epoch 45, Loss: 0.34860363602638245, Accuracy: 92.97366333007812\n",
      "Epoch 46, Loss: 0.3449532091617584, Accuracy: 93.02279663085938\n",
      "Epoch 47, Loss: 0.342428594827652, Accuracy: 92.96383666992188\n",
      "Epoch 48, Loss: 0.3411964476108551, Accuracy: 93.0621109008789\n",
      "Epoch 49, Loss: 0.3377958834171295, Accuracy: 93.13089752197266\n",
      "Epoch 50, Loss: 0.33558300137519836, Accuracy: 93.04244995117188\n",
      "Epoch 51, Loss: 0.3334272503852844, Accuracy: 93.1407241821289\n",
      "Epoch 52, Loss: 0.3301296830177307, Accuracy: 93.11124420166016\n",
      "Epoch 53, Loss: 0.32784196734428406, Accuracy: 93.1800308227539\n",
      "Epoch 54, Loss: 0.3257315456867218, Accuracy: 93.18985748291016\n",
      "Epoch 55, Loss: 0.32228559255599976, Accuracy: 93.20951080322266\n",
      "Epoch 56, Loss: 0.31957942247390747, Accuracy: 93.22917175292969\n",
      "Epoch 57, Loss: 0.3178999722003937, Accuracy: 93.24881744384766\n",
      "Epoch 58, Loss: 0.3150906562805176, Accuracy: 93.20951080322266\n",
      "Epoch 59, Loss: 0.3125646710395813, Accuracy: 93.32743835449219\n",
      "Epoch 60, Loss: 0.3112143874168396, Accuracy: 93.36674499511719\n",
      "Epoch 61, Loss: 0.3095260262489319, Accuracy: 93.32743835449219\n",
      "Epoch 62, Loss: 0.30676692724227905, Accuracy: 93.26847076416016\n",
      "Epoch 63, Loss: 0.3035951256752014, Accuracy: 93.40605163574219\n",
      "Epoch 64, Loss: 0.3018501102924347, Accuracy: 93.41587829589844\n",
      "Epoch 65, Loss: 0.2988378703594208, Accuracy: 93.43553161621094\n",
      "Epoch 66, Loss: 0.29634764790534973, Accuracy: 93.46501922607422\n",
      "Epoch 67, Loss: 0.29348739981651306, Accuracy: 93.49449920654297\n",
      "Epoch 68, Loss: 0.2908337414264679, Accuracy: 93.50431823730469\n",
      "Epoch 69, Loss: 0.28782373666763306, Accuracy: 93.55345916748047\n",
      "Epoch 70, Loss: 0.2859189510345459, Accuracy: 93.58293914794922\n",
      "Epoch 71, Loss: 0.2832113802433014, Accuracy: 93.66156005859375\n",
      "Epoch 72, Loss: 0.2811124622821808, Accuracy: 93.64189910888672\n",
      "Epoch 73, Loss: 0.27836644649505615, Accuracy: 93.6910400390625\n",
      "Epoch 74, Loss: 0.27441948652267456, Accuracy: 93.70085906982422\n",
      "Epoch 75, Loss: 0.2725650668144226, Accuracy: 93.88758087158203\n",
      "Epoch 76, Loss: 0.2698151171207428, Accuracy: 93.8089599609375\n",
      "Epoch 77, Loss: 0.2663266956806183, Accuracy: 93.79914093017578\n",
      "Epoch 78, Loss: 0.26392847299575806, Accuracy: 93.98584747314453\n",
      "Epoch 79, Loss: 0.26102012395858765, Accuracy: 94.00550079345703\n",
      "Epoch 80, Loss: 0.2581537663936615, Accuracy: 94.07428741455078\n",
      "Epoch 81, Loss: 0.25541621446609497, Accuracy: 94.04480743408203\n",
      "Epoch 82, Loss: 0.25192898511886597, Accuracy: 94.09394836425781\n",
      "Epoch 83, Loss: 0.2489059865474701, Accuracy: 94.23152923583984\n",
      "Epoch 84, Loss: 0.24556827545166016, Accuracy: 94.2806625366211\n",
      "Epoch 85, Loss: 0.2431529313325882, Accuracy: 94.27082824707031\n",
      "Epoch 86, Loss: 0.24024319648742676, Accuracy: 94.36910247802734\n",
      "Epoch 87, Loss: 0.2374240607023239, Accuracy: 94.3592758178711\n",
      "Epoch 88, Loss: 0.23471947014331818, Accuracy: 94.53616333007812\n",
      "Epoch 89, Loss: 0.23192568123340607, Accuracy: 94.55581665039062\n",
      "Epoch 90, Loss: 0.22877533733844757, Accuracy: 94.59513092041016\n",
      "Epoch 91, Loss: 0.22716668248176575, Accuracy: 94.7425308227539\n",
      "Epoch 92, Loss: 0.22366341948509216, Accuracy: 94.8407974243164\n",
      "Epoch 93, Loss: 0.22059547901153564, Accuracy: 94.83097076416016\n",
      "Epoch 94, Loss: 0.21735593676567078, Accuracy: 94.88993835449219\n",
      "Epoch 95, Loss: 0.21529999375343323, Accuracy: 95.01769256591797\n",
      "Epoch 96, Loss: 0.21246223151683807, Accuracy: 95.09630584716797\n",
      "Epoch 97, Loss: 0.2092055380344391, Accuracy: 95.14543914794922\n",
      "Epoch 98, Loss: 0.2058321237564087, Accuracy: 95.30267333984375\n",
      "Epoch 99, Loss: 0.2038605660200119, Accuracy: 95.36164093017578\n",
      "Epoch 100, Loss: 0.20077598094940186, Accuracy: 95.3321533203125\n",
      "Epoch 101, Loss: 0.1979893147945404, Accuracy: 95.45990753173828\n",
      "Epoch 102, Loss: 0.19463230669498444, Accuracy: 95.65644836425781\n",
      "Epoch 103, Loss: 0.19146497547626495, Accuracy: 95.72523498535156\n",
      "Epoch 104, Loss: 0.18928755819797516, Accuracy: 95.8431625366211\n",
      "Epoch 105, Loss: 0.186467245221138, Accuracy: 95.81368255615234\n",
      "Epoch 106, Loss: 0.18333709239959717, Accuracy: 96.01022338867188\n",
      "Epoch 107, Loss: 0.1809363216161728, Accuracy: 95.99057006835938\n",
      "Epoch 108, Loss: 0.17829149961471558, Accuracy: 96.15763092041016\n",
      "Epoch 109, Loss: 0.17415782809257507, Accuracy: 96.23624420166016\n",
      "Epoch 110, Loss: 0.1710798740386963, Accuracy: 96.3639907836914\n",
      "Epoch 111, Loss: 0.16826827824115753, Accuracy: 96.33451080322266\n",
      "Epoch 112, Loss: 0.16397224366664886, Accuracy: 96.48191833496094\n",
      "Epoch 113, Loss: 0.1610032618045807, Accuracy: 96.57035827636719\n",
      "Epoch 114, Loss: 0.15786689519882202, Accuracy: 96.68828582763672\n",
      "Epoch 115, Loss: 0.15389807522296906, Accuracy: 96.86517333984375\n",
      "Epoch 116, Loss: 0.15076610445976257, Accuracy: 96.92414093017578\n",
      "Epoch 117, Loss: 0.1475743055343628, Accuracy: 97.09119415283203\n",
      "Epoch 118, Loss: 0.1441841423511505, Accuracy: 97.22877502441406\n",
      "Epoch 119, Loss: 0.1398095190525055, Accuracy: 97.28773498535156\n",
      "Epoch 120, Loss: 0.1373957395553589, Accuracy: 97.15016174316406\n",
      "Epoch 121, Loss: 0.13436448574066162, Accuracy: 97.3860092163086\n",
      "Epoch 122, Loss: 0.13094493746757507, Accuracy: 97.4842758178711\n",
      "Epoch 123, Loss: 0.127179816365242, Accuracy: 97.57272338867188\n",
      "Epoch 124, Loss: 0.12339182198047638, Accuracy: 97.61203002929688\n",
      "Epoch 125, Loss: 0.12002313137054443, Accuracy: 97.65133666992188\n",
      "Epoch 126, Loss: 0.11679218709468842, Accuracy: 97.73977661132812\n",
      "Epoch 127, Loss: 0.11426553875207901, Accuracy: 97.81839752197266\n",
      "Epoch 128, Loss: 0.11030731350183487, Accuracy: 97.97563171386719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Loss: 0.10788575559854507, Accuracy: 97.97563171386719\n",
      "Epoch 130, Loss: 0.10573737323284149, Accuracy: 98.04441833496094\n",
      "Epoch 131, Loss: 0.10231230407953262, Accuracy: 98.11321258544922\n",
      "Epoch 132, Loss: 0.09878571331501007, Accuracy: 98.13285827636719\n",
      "Epoch 133, Loss: 0.09604290872812271, Accuracy: 98.19181823730469\n",
      "Epoch 134, Loss: 0.0934770479798317, Accuracy: 98.24095916748047\n",
      "Epoch 135, Loss: 0.0909980908036232, Accuracy: 98.3392333984375\n",
      "Epoch 136, Loss: 0.08784918487071991, Accuracy: 98.35887908935547\n",
      "Epoch 137, Loss: 0.08556298911571503, Accuracy: 98.3785400390625\n",
      "Epoch 138, Loss: 0.08323968946933746, Accuracy: 98.3785400390625\n",
      "Epoch 139, Loss: 0.08114688843488693, Accuracy: 98.4571533203125\n",
      "Epoch 140, Loss: 0.0787009745836258, Accuracy: 98.4964599609375\n",
      "Epoch 141, Loss: 0.07576128840446472, Accuracy: 98.52593994140625\n",
      "Epoch 142, Loss: 0.07437480986118317, Accuracy: 98.63404083251953\n",
      "Epoch 143, Loss: 0.07074864953756332, Accuracy: 98.73230743408203\n",
      "Epoch 144, Loss: 0.068190798163414, Accuracy: 98.75196838378906\n",
      "Epoch 145, Loss: 0.0663585290312767, Accuracy: 98.79127502441406\n",
      "Epoch 146, Loss: 0.06416081637144089, Accuracy: 98.83058166503906\n",
      "Epoch 147, Loss: 0.061625886708498, Accuracy: 98.86988830566406\n",
      "Epoch 148, Loss: 0.05984612554311752, Accuracy: 98.91902923583984\n",
      "Epoch 149, Loss: 0.05730042606592178, Accuracy: 98.93868255615234\n",
      "Epoch 150, Loss: 0.05560221150517464, Accuracy: 98.95832824707031\n",
      "Epoch 151, Loss: 0.05377129465341568, Accuracy: 98.99764251708984\n",
      "Epoch 152, Loss: 0.05223521590232849, Accuracy: 99.0271224975586\n",
      "Epoch 153, Loss: 0.05049688741564751, Accuracy: 99.0664291381836\n",
      "Epoch 154, Loss: 0.0487862154841423, Accuracy: 99.0664291381836\n",
      "Epoch 155, Loss: 0.04671511799097061, Accuracy: 99.10574340820312\n",
      "Epoch 156, Loss: 0.04561557248234749, Accuracy: 99.10574340820312\n",
      "Epoch 157, Loss: 0.04450263828039169, Accuracy: 99.10574340820312\n",
      "Epoch 158, Loss: 0.042713213711977005, Accuracy: 99.20401000976562\n",
      "Epoch 159, Loss: 0.041437409818172455, Accuracy: 99.20401000976562\n",
      "Epoch 160, Loss: 0.040058329701423645, Accuracy: 99.27279663085938\n",
      "Epoch 161, Loss: 0.03861173242330551, Accuracy: 99.28263092041016\n",
      "Epoch 162, Loss: 0.03667920082807541, Accuracy: 99.29244995117188\n",
      "Epoch 163, Loss: 0.03580157831311226, Accuracy: 99.3710708618164\n",
      "Epoch 164, Loss: 0.035017333924770355, Accuracy: 99.34159088134766\n",
      "Epoch 165, Loss: 0.033799976110458374, Accuracy: 99.3710708618164\n",
      "Epoch 166, Loss: 0.03326396271586418, Accuracy: 99.4693374633789\n",
      "Epoch 167, Loss: 0.03379429131746292, Accuracy: 99.42020416259766\n",
      "Epoch 168, Loss: 0.032989077270030975, Accuracy: 99.38089752197266\n",
      "Epoch 169, Loss: 0.03105076029896736, Accuracy: 99.43985748291016\n",
      "Epoch 170, Loss: 0.029601234942674637, Accuracy: 99.53813171386719\n",
      "Epoch 171, Loss: 0.028165150433778763, Accuracy: 99.49881744384766\n",
      "Epoch 172, Loss: 0.026956304907798767, Accuracy: 99.57743835449219\n",
      "Epoch 173, Loss: 0.02629135176539421, Accuracy: 99.62657165527344\n",
      "Epoch 174, Loss: 0.024788761511445045, Accuracy: 99.63639831542969\n",
      "Epoch 175, Loss: 0.023511890321969986, Accuracy: 99.68553161621094\n",
      "Epoch 176, Loss: 0.022806819528341293, Accuracy: 99.70519256591797\n",
      "Epoch 177, Loss: 0.02184232696890831, Accuracy: 99.72483825683594\n",
      "Epoch 178, Loss: 0.02114025130867958, Accuracy: 99.77397918701172\n",
      "Epoch 179, Loss: 0.02043718472123146, Accuracy: 99.78380584716797\n",
      "Epoch 180, Loss: 0.019706513732671738, Accuracy: 99.79363250732422\n",
      "Epoch 181, Loss: 0.018807299435138702, Accuracy: 99.81328582763672\n",
      "Epoch 182, Loss: 0.018135109916329384, Accuracy: 99.81328582763672\n",
      "Epoch 183, Loss: 0.017402201890945435, Accuracy: 99.83293914794922\n",
      "Epoch 184, Loss: 0.017252955585718155, Accuracy: 99.82311248779297\n",
      "Epoch 185, Loss: 0.01665189489722252, Accuracy: 99.85259246826172\n",
      "Epoch 186, Loss: 0.01580856181681156, Accuracy: 99.86241912841797\n",
      "Epoch 187, Loss: 0.015404894948005676, Accuracy: 99.87224578857422\n",
      "Epoch 188, Loss: 0.015373860485851765, Accuracy: 99.83293914794922\n",
      "Epoch 189, Loss: 0.014921003952622414, Accuracy: 99.9017333984375\n",
      "Epoch 190, Loss: 0.014264101162552834, Accuracy: 99.91156005859375\n",
      "Epoch 191, Loss: 0.013798283413052559, Accuracy: 99.89189910888672\n",
      "Epoch 192, Loss: 0.013437418267130852, Accuracy: 99.91156005859375\n",
      "Epoch 193, Loss: 0.013441423885524273, Accuracy: 99.882080078125\n",
      "Epoch 194, Loss: 0.012697480618953705, Accuracy: 99.9017333984375\n",
      "Epoch 195, Loss: 0.0121805714443326, Accuracy: 99.93121337890625\n",
      "Epoch 196, Loss: 0.012008944526314735, Accuracy: 99.9017333984375\n",
      "Epoch 197, Loss: 0.01130328793078661, Accuracy: 99.91156005859375\n",
      "Epoch 198, Loss: 0.01111556589603424, Accuracy: 99.93121337890625\n",
      "Epoch 199, Loss: 0.01066744327545166, Accuracy: 99.95085906982422\n",
      "Epoch 200, Loss: 0.010347500443458557, Accuracy: 99.9410400390625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for seqs, labels in train_ds:\n",
    "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100))\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Question :  ['저 다 에 되나요 \\n']\n",
      "Real Answer :  ['\\t 네 에 됩니다 \\n']\n",
      "Prediction Answer:  ['와이파이 암호 는 가나다라 입니다 \\n']\n",
      "_\n",
      "Question :  ['아이스 카푸치노 컵 에 주실 수 있나요 \\n']\n",
      "Real Answer :  ['\\t 컵 사이즈 가 음료 가 다 안 \\n']\n",
      "Prediction Answer:  ['네 가능합니다 \\n']\n",
      "_\n",
      "Question :  ['네 이 할인 사용 할게요 \\n']\n",
      "Real Answer :  ['\\t 네 드시고 가시나요 \\n']\n",
      "Prediction Answer:  ['네 500원 할인 되세요 \\n']\n",
      "_\n",
      "Question :  ['와이파이 비밀번호 는 뭐 에요 \\n']\n",
      "Real Answer :  ['\\t 에 \\n']\n",
      "Prediction Answer:  ['와이파이 비밀번호 는 종이 에 써져있습니다 \\n']\n",
      "_\n",
      "Question :  ['밀크 티 있나요 \\n']\n",
      "Real Answer :  ['\\t 네 있습니다 \\n']\n",
      "Prediction Answer:  ['네 캐리어 에 2 나왔습니다 \\n']\n",
      "_\n",
      "Question :  ['밀크 티 에 혹시 우유 가 우유 이 \\n']\n",
      "Real Answer :  ['\\t 저희 는 우유 을 사용 하고 있어요 \\n']\n",
      "Prediction Answer:  ['유리잔 영업 으로 드릴가요 \\n']\n",
      "_\n",
      "Question :  ['쿠폰 주시나요 \\n']\n",
      "Real Answer :  ['\\t 네 드릴게요 만 요 \\n']\n",
      "Prediction Answer:  ['네 카드 받았습니다 \\n']\n",
      "_\n",
      "Question :  ['아이스 아메리카노 한잔 이 요 \\n']\n",
      "Real Answer :  ['\\t 드시고 \\n']\n",
      "Prediction Answer:  ['네 아이스 아메리카노 로 드릴 까요 \\n']\n",
      "_\n",
      "Question :  ['번 에 로 하고 찍어주세요 \\n']\n",
      "Real Answer :  ['\\t 네 \\n']\n",
      "Prediction Answer:  ['네 캐리어 에 와 드릴게요 \\n']\n",
      "_\n",
      "Question :  ['음료 랑 다 해서 얼마 에요 \\n']\n",
      "Real Answer :  ['\\t 500원 입니다 \\n']\n",
      "Prediction Answer:  ['네 어떤 와 로 드릴 까요 \\n']\n",
      "_\n",
      "Question :  ['아메리카노 는 어떤 종류 가 있나요 \\n']\n",
      "Real Answer :  ['\\t 디카 페인 과 아메리카노 2 종류 있습니다 \\n']\n",
      "Prediction Answer:  ['딸기 주스 바나나 주스 보다 키위 주스 \\n']\n",
      "_\n",
      "Question :  ['포인트 적립 같이 할 수 있나요 \\n']\n",
      "Real Answer :  ['\\t 네 같이 적립 해 드리겠습니다 \\n']\n",
      "Prediction Answer:  ['네 네 메뉴 메뉴 기계 \\n']\n",
      "_\n",
      "Question :  ['해주세요 \\n']\n",
      "Real Answer :  ['\\t 네 번호 찍어주세요 \\n']\n",
      "Prediction Answer:  ['네 저희 매장 에서는 디카 페인 과 하프 디카 페인 중 에 선택 하실 수 있습니다 \\n']\n",
      "_\n",
      "Question :  ['주문 해 요 \\n']\n",
      "Real Answer :  ['\\t 네 메뉴 는 어떻게 드릴 까요 \\n']\n",
      "Prediction Answer:  ['네 아이스 아메리카노 로 한 잔 드릴게요 \\n']\n",
      "_\n",
      "Question :  ['아아 주문 가능한가요 \\n']\n",
      "Real Answer :  ['\\t 네 가능합니다 \\n']\n",
      "Prediction Answer:  ['네 가능합니다 \\n']\n",
      "_\n",
      "Question :  ['혹시 커피 도 판매 하나요 \\n']\n",
      "Real Answer :  ['\\t 아니요 커피 는 판매 \\n']\n",
      "Prediction Answer:  ['네 저희 매장 진동 벨 로 알려 드릴게요 \\n']\n",
      "_\n",
      "Question :  ['에서 사용 가능한가요 \\n']\n",
      "Real Answer :  ['\\t 아뇨 으로 하고 있어요 \\n']\n",
      "Prediction Answer:  ['아뇨 매장 에서는 머그컵 만 사용 가능합니다 \\n']\n",
      "_\n",
      "Question :  ['한 한잔 하나요 \\n']\n",
      "Real Answer :  ['\\t 저희 매장 은 잔 이 입니다 \\n']\n",
      "Prediction Answer:  ['중앙 가시나요 \\n']\n",
      "_\n",
      "Question :  ['차갑게 주세요 \\n']\n",
      "Real Answer :  ['\\t 입니다 \\n']\n",
      "Prediction Answer:  ['네 가능합니다 \\n']\n",
      "_\n",
      "Question :  ['카페라테 한 잔 주세요 \\n']\n",
      "Real Answer :  ['\\t 카페라테 따뜻한 걸 로 드릴 까요 \\n']\n",
      "Prediction Answer:  ['네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n",
      "_\n",
      "Question :  ['카페라테 디카 페인 \\n']\n",
      "Real Answer :  ['\\t 네 \\n']\n",
      "Prediction Answer:  ['네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n",
      "_\n",
      "Question :  ['자몽 에 이드 한 잔 얼마 에요 \\n']\n",
      "Real Answer :  ['\\t \\n']\n",
      "Prediction Answer:  ['네 2 에 와 500원 잔 에 잔 으로 구성 된 세트 있습니다 \\n']\n",
      "_\n",
      "Question :  ['생크림 케이크 는 뭐 가 제일 \\n']\n",
      "Real Answer :  ['\\t 과일 생크림 케이크 가 잘 \\n']\n",
      "Prediction Answer:  ['네 배달 비 3000원 입니다 \\n']\n",
      "_\n",
      "Question :  ['아이스 프라푸치노 주세요 \\n']\n",
      "Real Answer :  ['\\t 네 아이스 프라푸치노 몇 잔 드릴 까요 \\n']\n",
      "Prediction Answer:  ['네 쿠폰 찍어 드릴 까요 \\n']\n",
      "_\n",
      "Question :  ['네 화장실 은 어디 에요 \\n']\n",
      "Real Answer :  ['\\t 화장실 은 로 면 \\n']\n",
      "Prediction Answer:  ['네 다 되면 딸기 스무디 와 드릴게요 \\n']\n",
      "_\n",
      "Question :  ['딸기 스무디 되나요 \\n']\n",
      "Real Answer :  ['\\t 지금 은 가 다 딸기 스무디 는 \\n']\n",
      "Prediction Answer:  ['머그잔 으로 제공 해드려도 괜찮을까요 \\n']\n",
      "_\n",
      "Question :  ['이 기프티콘 으로 할 수 있는 시즌 메뉴 는 인가요 \\n']\n",
      "Real Answer :  ['\\t 기프티콘 으로 할 수 있는 시즌 메뉴 는 \\n']\n",
      "Prediction Answer:  ['7천원 입니다 \\n']\n",
      "_\n",
      "Question :  ['카페라테 라지 로 주세요 \\n']\n",
      "Real Answer :  ['\\t 드시고 건가 요 \\n']\n",
      "Prediction Answer:  ['네 카페라테 디카 페인 추가 됩니다 \\n']\n",
      "_\n",
      "Question :  ['이 치즈케이크 도 한 조각 주세요 \\n']\n",
      "Real Answer :  ['\\t 여기 서 건가 요 \\n']\n",
      "Prediction Answer:  ['네 총 9500원 결제 해드리겠습니다 \\n']\n",
      "_\n",
      "Question :  ['카페인 이 음료 있나요 \\n']\n",
      "Real Answer :  ['\\t 티 음료 와 스무디 에는 카페인 이 \\n']\n",
      "Prediction Answer:  ['네 주문 디카 페인 사이즈 블랙 티 를 우려 서 만들고 있어요 \\n']\n",
      "_\n",
      "Question :  ['아메리카노 한 잔이요 \\n']\n",
      "Real Answer :  ['\\t 아이스 아메리카노 로 드릴 까요 \\n']\n",
      "Prediction Answer:  ['드시고 가시나요 \\n']\n",
      "_\n",
      "Question :  ['디카 페인 아이스 아메리카노 한 잔 주세요 \\n']\n",
      "Real Answer :  ['\\t 디카 페인 아이스 아메리카노 는 에 추가 괜찮으신 가요 \\n']\n",
      "Prediction Answer:  ['아이스 아메리카노 포장 이신 가요 \\n']\n",
      "_\n",
      "Question :  ['\\n']\n",
      "Real Answer :  ['\\t 결제 여기 있습니다 \\n']\n",
      "Prediction Answer:  ['바닐라 라테 따뜻한 거 요 \\n']\n",
      "_\n",
      "Question :  ['커피 음료 것 뭐 가 있나요 \\n']\n",
      "Real Answer :  ['\\t 스무디 와 주스 있습니다 \\n']\n",
      "Prediction Answer:  ['네 알겠습니다 \\n']\n",
      "_\n",
      "Question :  ['딸기 주스 로 주세요 \\n']\n",
      "Real Answer :  ['\\t 네 더 필요한 건 없으세요 \\n']\n",
      "Prediction Answer:  ['네 는 음료 로 드릴 까요 \\n']\n",
      "_\n",
      "Question :  ['따뜻한 밀크 티 주세요 \\n']\n",
      "Real Answer :  ['\\t 네 \\n']\n",
      "Prediction Answer:  ['쿠폰 주시 면 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "Question :  ['아이스 아메리카노 한 잔 주문 할게요 \\n']\n",
      "Real Answer :  ['\\t 네 알겠습니다 \\n']\n",
      "Prediction Answer:  ['아이스 아메리카노 에 샷 은 세 개 들어갑니다 \\n']\n",
      "_\n",
      "Question :  ['치즈 케이크 는 지금 없나요 \\n']\n",
      "Real Answer :  ['\\t 네 치즈케이크 는 지금 다 \\n']\n",
      "Prediction Answer:  ['네 어떤 걸 로 하시겠습니까 \\n']\n",
      "_\n",
      "Question :  ['현금영수증 번호 \\n']\n",
      "Real Answer :  ['\\t 네 \\n']\n",
      "Prediction Answer:  ['바닐라 라테 따뜻한 거 요 \\n']\n",
      "_\n",
      "Question :  ['아이스 아메리카노 두 잔 주세요 \\n']\n",
      "Real Answer :  ['\\t 네 총 입니다 \\n']\n",
      "Prediction Answer:  ['드시고 가시나요 \\n']\n",
      "_\n",
      "Question :  ['는 몇 시 까지 하나요 \\n']\n",
      "Real Answer :  ['\\t 까지 \\n']\n",
      "Prediction Answer:  ['매장 영업 시간 은 12시 까지라 지금 은 지금 만 가능하세요 \\n']\n"
     ]
    }
   ],
   "source": [
    "for test_seq, test_labels in test_ds:\n",
    "    prediction = test_step(model, test_seq)\n",
    "    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
    "    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
    "    texts = tokenizer.sequences_to_texts(prediction.numpy())\n",
    "    print('_')\n",
    "    print('Question : ', test_text)\n",
    "    print('Real Answer : ', gt_text)\n",
    "    print('Prediction Answer: ', texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
